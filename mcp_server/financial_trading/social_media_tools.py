"""
Social Media Tools for Financial Trading MCP Server
Provides tools for sentiment analysis from Reddit, Twitter/X, and other social platforms
"""
import os
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import httpx
import re

# Configuration
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID", "")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET", "")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "TradingBot/1.0")
X_BEARER_TOKEN = os.getenv("X_BEARER_TOKEN", "")  # Twitter/X API v2
EODHD_API_KEY = os.getenv("EODHD_API_KEY", "")

# Ticker to company name mapping
TICKER_TO_COMPANY = {
    "AAPL": "Apple",
    "MSFT": "Microsoft",
    "GOOGL": "Google",
    "AMZN": "Amazon",
    "TSLA": "Tesla",
    "NVDA": "Nvidia",
    "TSM": "Taiwan Semiconductor OR TSMC",
    "JPM": "JPMorgan Chase OR JP Morgan",
    "JNJ": "Johnson & Johnson",
    "V": "Visa",
    "WMT": "Walmart",
    "META": "Meta OR Facebook",
    "AMD": "AMD",
    "INTC": "Intel",
    "QCOM": "Qualcomm",
    "BABA": "Alibaba",
    "ADBE": "Adobe",
    "NFLX": "Netflix",
    "CRM": "Salesforce",
    "PYPL": "PayPal",
    "PLTR": "Palantir",
    "MU": "Micron",
    "SQ": "Block OR Square",
    "ZM": "Zoom",
    "CSCO": "Cisco",
    "SHOP": "Shopify",
    "ORCL": "Oracle",
    "SPOT": "Spotify",
    "AVGO": "Broadcom",
    "ASML": "ASML",
    "UBER": "Uber",
    "ROKU": "Roku",
    "PINS": "Pinterest",
}

async def get_http() -> httpx.AsyncClient:
    """Get or create HTTP client."""
    return httpx.AsyncClient(timeout=30.0)

async def get_reddit_sentiment(
    ticker: str,
    subreddits: List[str] = None,
    limit: int = 25
) -> str:
    """
    Get sentiment analysis from Reddit posts about a stock.
    
    Args:
        ticker: Ticker symbol (e.g., AAPL, TSLA)
        subreddits: List of subreddits to search (default: wallstreetbets, stocks, investing)
        limit: Maximum number of posts to analyze per subreddit
    
    Returns:
        JSON string with Reddit posts, sentiment scores, and analysis
    """
    try:
        if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET:
            return json.dumps({
                "error": "Reddit API credentials not configured. Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET"
            })
        
        if subreddits is None:
            subreddits = ["wallstreetbets", "stocks", "investing", "StockMarket", "options"]
        
        client = await get_http()
        
        # Get Reddit access token
        auth_response = await client.post(
            "https://www.reddit.com/api/v1/access_token",
            auth=(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET),
            data={"grant_type": "client_credentials"},
            headers={"User-Agent": REDDIT_USER_AGENT}
        )
        
        if auth_response.status_code != 200:
            return json.dumps({"error": "Failed to authenticate with Reddit API"})
        
        access_token = auth_response.json()["access_token"]
        headers = {
            "Authorization": f"Bearer {access_token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        all_posts = []
        company_names = TICKER_TO_COMPANY.get(ticker, ticker)
        search_terms = [ticker]
        
        if " OR " in company_names:
            search_terms.extend(company_names.split(" OR "))
        else:
            search_terms.append(company_names)
        
        for subreddit in subreddits:
            for term in search_terms:
                try:
                    # Search for posts mentioning the ticker/company
                    search_url = f"https://oauth.reddit.com/r/{subreddit}/search.json"
                    params = {
                        "q": term,
                        "restrict_sr": "true",
                        "sort": "new",
                        "limit": limit,
                        "t": "week"  # Time filter: past week
                    }
                    
                    response = await client.get(search_url, headers=headers, params=params)
                    
                    if response.status_code == 200:
                        data = response.json()
                        posts = data.get("data", {}).get("children", [])
                        
                        for post in posts:
                            post_data = post.get("data", {})
                            
                            # Simple sentiment analysis based on keywords
                            title = post_data.get("title", "").lower()
                            selftext = post_data.get("selftext", "").lower()
                            full_text = title + " " + selftext
                            
                            # Bullish keywords
                            bullish_words = ["buy", "calls", "moon", "rocket", "bull", "long", "pump", 
                                           "squeeze", "gain", "profit", "up", "green", "breakout", "rally"]
                            # Bearish keywords
                            bearish_words = ["sell", "puts", "crash", "bear", "short", "dump", 
                                           "loss", "down", "red", "collapse", "decline", "fall"]
                            
                            bullish_score = sum(1 for word in bullish_words if word in full_text)
                            bearish_score = sum(1 for word in bearish_words if word in full_text)
                            
                            sentiment = "neutral"
                            if bullish_score > bearish_score:
                                sentiment = "bullish"
                            elif bearish_score > bullish_score:
                                sentiment = "bearish"
                            
                            all_posts.append({
                                "subreddit": subreddit,
                                "title": post_data.get("title"),
                                "author": post_data.get("author"),
                                "score": post_data.get("score"),
                                "upvote_ratio": post_data.get("upvote_ratio"),
                                "num_comments": post_data.get("num_comments"),
                                "created_utc": datetime.fromtimestamp(post_data.get("created_utc", 0)).strftime("%Y-%m-%d %H:%M:%S"),
                                "url": f"https://reddit.com{post_data.get('permalink', '')}",
                                "sentiment": sentiment,
                                "bullish_score": bullish_score,
                                "bearish_score": bearish_score
                            })
                except Exception as e:
                    continue
        
        # Sort by score (upvotes)
        all_posts.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # Calculate overall sentiment
        total_bullish = sum(1 for p in all_posts if p["sentiment"] == "bullish")
        total_bearish = sum(1 for p in all_posts if p["sentiment"] == "bearish")
        total_neutral = sum(1 for p in all_posts if p["sentiment"] == "neutral")
        
        overall_sentiment = "neutral"
        if total_bullish > total_bearish * 1.5:
            overall_sentiment = "bullish"
        elif total_bearish > total_bullish * 1.5:
            overall_sentiment = "bearish"
        
        # Calculate weighted sentiment based on upvotes
        weighted_bullish = sum(p.get("score", 0) for p in all_posts if p["sentiment"] == "bullish")
        weighted_bearish = sum(p.get("score", 0) for p in all_posts if p["sentiment"] == "bearish")
        
        result = {
            "symbol": ticker,
            "source": "reddit",
            "posts_analyzed": len(all_posts),
            "overall_sentiment": overall_sentiment,
            "sentiment_breakdown": {
                "bullish": total_bullish,
                "bearish": total_bearish,
                "neutral": total_neutral
            },
            "weighted_sentiment": {
                "bullish_score": weighted_bullish,
                "bearish_score": weighted_bearish,
                "ratio": round(weighted_bullish / (weighted_bearish + 1), 2)  # Avoid division by zero
            },
            "top_posts": all_posts[:10],  # Return top 10 posts
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_twitter_sentiment(
    ticker: str,
    limit: int = 50
) -> str:
    """
    Get sentiment analysis from Twitter/X posts about a stock.
    
    Args:
        ticker: Ticker symbol (e.g., AAPL, TSLA)
        limit: Maximum number of tweets to analyze
    
    Returns:
        JSON string with Twitter posts, sentiment scores, and analysis
    """
    try:
        if not X_BEARER_TOKEN:
            return json.dumps({
                "error": "Twitter/X API token not configured. Set X_BEARER_TOKEN environment variable"
            })
        
        client = await get_http()
        headers = {
            "Authorization": f"Bearer {X_BEARER_TOKEN}",
            "Content-Type": "application/json"
        }
        
        # Build search query
        company_name = TICKER_TO_COMPANY.get(ticker, ticker)
        query = f"${ticker} OR {company_name} -is:retweet lang:en"
        
        # Search recent tweets
        search_url = "https://api.twitter.com/2/tweets/search/recent"
        params = {
            "query": query,
            "max_results": min(limit, 100),
            "tweet.fields": "created_at,author_id,public_metrics,context_annotations",
            "expansions": "author_id"
        }
        
        response = await client.get(search_url, headers=headers, params=params)
        
        if response.status_code != 200:
            return json.dumps({
                "error": f"Twitter API error: {response.status_code}",
                "details": response.text
            })
        
        data = response.json()
        tweets = data.get("data", [])
        users = {u["id"]: u for u in data.get("includes", {}).get("users", [])}
        
        analyzed_tweets = []
        
        for tweet in tweets:
            text = tweet.get("text", "").lower()
            
            # Simple sentiment analysis
            bullish_words = ["buy", "calls", "moon", "rocket", "bull", "long", "pump", 
                           "squeeze", "gain", "profit", "up", "green", "breakout", "rally", "ðŸš€", "ðŸ“ˆ", "ðŸ’Ž", "ðŸ™Œ"]
            bearish_words = ["sell", "puts", "crash", "bear", "short", "dump", 
                           "loss", "down", "red", "collapse", "decline", "fall", "ðŸ“‰", "ðŸ”»", "ðŸ’”"]
            
            bullish_score = sum(1 for word in bullish_words if word in text)
            bearish_score = sum(1 for word in bearish_words if word in text)
            
            sentiment = "neutral"
            if bullish_score > bearish_score:
                sentiment = "bullish"
            elif bearish_score > bullish_score:
                sentiment = "bearish"
            
            metrics = tweet.get("public_metrics", {})
            author = users.get(tweet.get("author_id", ""), {})
            
            analyzed_tweets.append({
                "id": tweet.get("id"),
                "text": tweet.get("text"),
                "author": author.get("username", "unknown"),
                "created_at": tweet.get("created_at"),
                "likes": metrics.get("like_count", 0),
                "retweets": metrics.get("retweet_count", 0),
                "replies": metrics.get("reply_count", 0),
                "sentiment": sentiment,
                "bullish_score": bullish_score,
                "bearish_score": bearish_score,
                "engagement": metrics.get("like_count", 0) + metrics.get("retweet_count", 0)
            })
        
        # Sort by engagement
        analyzed_tweets.sort(key=lambda x: x.get("engagement", 0), reverse=True)
        
        # Calculate overall sentiment
        total_bullish = sum(1 for t in analyzed_tweets if t["sentiment"] == "bullish")
        total_bearish = sum(1 for t in analyzed_tweets if t["sentiment"] == "bearish")
        total_neutral = sum(1 for t in analyzed_tweets if t["sentiment"] == "neutral")
        
        overall_sentiment = "neutral"
        if total_bullish > total_bearish * 1.5:
            overall_sentiment = "bullish"
        elif total_bearish > total_bullish * 1.5:
            overall_sentiment = "bearish"
        
        # Calculate weighted sentiment based on engagement
        weighted_bullish = sum(t.get("engagement", 0) for t in analyzed_tweets if t["sentiment"] == "bullish")
        weighted_bearish = sum(t.get("engagement", 0) for t in analyzed_tweets if t["sentiment"] == "bearish")
        
        result = {
            "symbol": ticker,
            "source": "twitter",
            "tweets_analyzed": len(analyzed_tweets),
            "overall_sentiment": overall_sentiment,
            "sentiment_breakdown": {
                "bullish": total_bullish,
                "bearish": total_bearish,
                "neutral": total_neutral
            },
            "weighted_sentiment": {
                "bullish_score": weighted_bullish,
                "bearish_score": weighted_bearish,
                "ratio": round(weighted_bullish / (weighted_bearish + 1), 2)
            },
            "top_tweets": analyzed_tweets[:10],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_social_sentiment_summary(
    ticker: str,
    platforms: List[str] = None
) -> str:
    """
    Get aggregated sentiment analysis from multiple social media platforms.
    
    Args:
        ticker: Ticker symbol (e.g., AAPL, TSLA)
        platforms: List of platforms to analyze (default: ["reddit", "twitter"])
    
    Returns:
        JSON string with aggregated sentiment analysis from all platforms
    """
    try:
        if platforms is None:
            platforms = ["reddit", "twitter"]
        
        results = {
            "symbol": ticker,
            "platforms": {},
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        total_bullish = 0
        total_bearish = 0
        total_neutral = 0
        
        # Get sentiment from each platform
        for platform in platforms:
            if platform == "reddit":
                reddit_result = await get_reddit_sentiment(ticker, limit=25)
                reddit_data = json.loads(reddit_result)
                if "error" not in reddit_data:
                    results["platforms"]["reddit"] = reddit_data
                    breakdown = reddit_data.get("sentiment_breakdown", {})
                    total_bullish += breakdown.get("bullish", 0)
                    total_bearish += breakdown.get("bearish", 0)
                    total_neutral += breakdown.get("neutral", 0)
            
            elif platform == "twitter":
                twitter_result = await get_twitter_sentiment(ticker, limit=50)
                twitter_data = json.loads(twitter_result)
                if "error" not in twitter_data:
                    results["platforms"]["twitter"] = twitter_data
                    breakdown = twitter_data.get("sentiment_breakdown", {})
                    total_bullish += breakdown.get("bullish", 0)
                    total_bearish += breakdown.get("bearish", 0)
                    total_neutral += breakdown.get("neutral", 0)
        
        # Calculate overall sentiment
        total_posts = total_bullish + total_bearish + total_neutral
        
        if total_posts > 0:
            bullish_percent = (total_bullish / total_posts) * 100
            bearish_percent = (total_bearish / total_posts) * 100
            neutral_percent = (total_neutral / total_posts) * 100
            
            overall_sentiment = "neutral"
            if bullish_percent > 50:
                overall_sentiment = "bullish"
            elif bearish_percent > 50:
                overall_sentiment = "bearish"
            
            results["aggregated_sentiment"] = {
                "overall": overall_sentiment,
                "bullish_percent": round(bullish_percent, 1),
                "bearish_percent": round(bearish_percent, 1),
                "neutral_percent": round(neutral_percent, 1),
                "total_posts_analyzed": total_posts,
                "confidence": "high" if total_posts > 50 else "medium" if total_posts > 20 else "low"
            }
            
            # Generate trading signal
            if bullish_percent > 60:
                results["trading_signal"] = {
                    "action": "BUY",
                    "strength": "strong" if bullish_percent > 70 else "moderate",
                    "reason": f"Strong bullish sentiment ({bullish_percent:.1f}%) across social media"
                }
            elif bearish_percent > 60:
                results["trading_signal"] = {
                    "action": "SELL",
                    "strength": "strong" if bearish_percent > 70 else "moderate",
                    "reason": f"Strong bearish sentiment ({bearish_percent:.1f}%) across social media"
                }
            else:
                results["trading_signal"] = {
                    "action": "HOLD",
                    "strength": "neutral",
                    "reason": "Mixed or neutral sentiment across social media"
                }
        else:
            results["aggregated_sentiment"] = {
                "overall": "no_data",
                "message": "Insufficient social media data for analysis"
            }
        
        return json.dumps(results, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_trending_tickers(
    platforms: List[str] = None,
    limit: int = 10
) -> str:
    """
    Get trending stock tickers from social media platforms.
    
    Args:
        platforms: List of platforms to check (default: ["reddit"])
        limit: Maximum number of trending tickers to return
    
    Returns:
        JSON string with trending tickers and their mention counts
    """
    try:
        if platforms is None:
            platforms = ["reddit"]
        
        trending = {}
        
        for platform in platforms:
            if platform == "reddit" and REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET:
                client = await get_http()
                
                # Get Reddit access token
                auth_response = await client.post(
                    "https://www.reddit.com/api/v1/access_token",
                    auth=(REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET),
                    data={"grant_type": "client_credentials"},
                    headers={"User-Agent": REDDIT_USER_AGENT}
                )
                
                if auth_response.status_code == 200:
                    access_token = auth_response.json()["access_token"]
                    headers = {
                        "Authorization": f"Bearer {access_token}",
                        "User-Agent": REDDIT_USER_AGENT
                    }
                    
                    # Get hot posts from investing subreddits
                    subreddits = ["wallstreetbets", "stocks", "investing"]
                    
                    for subreddit in subreddits:
                        url = f"https://oauth.reddit.com/r/{subreddit}/hot.json"
                        params = {"limit": 50}
                        
                        response = await client.get(url, headers=headers, params=params)
                        
                        if response.status_code == 200:
                            data = response.json()
                            posts = data.get("data", {}).get("children", [])
                            
                            for post in posts:
                                post_data = post.get("data", {})
                                title = post_data.get("title", "")
                                selftext = post_data.get("selftext", "")
                                full_text = title + " " + selftext
                                
                                # Extract ticker symbols (format: $TICKER or TICKER in uppercase)
                                ticker_pattern = r'\$([A-Z]{1,5})\b|(?:^|\s)([A-Z]{2,5})(?:\s|$)'
                                matches = re.findall(ticker_pattern, full_text)
                                
                                for match in matches:
                                    ticker = match[0] if match[0] else match[1]
                                    if ticker and len(ticker) >= 2 and len(ticker) <= 5:
                                        # Filter out common words that might be mistaken for tickers
                                        common_words = ["THE", "AND", "FOR", "ARE", "BUT", "NOT", "YOU", "ALL", 
                                                      "CAN", "HER", "WAS", "ONE", "OUR", "OUT", "DAY", "GET"]
                                        if ticker not in common_words:
                                            if ticker not in trending:
                                                trending[ticker] = {
                                                    "mentions": 0,
                                                    "total_score": 0,
                                                    "platforms": set()
                                                }
                                            trending[ticker]["mentions"] += 1
                                            trending[ticker]["total_score"] += post_data.get("score", 0)
                                            trending[ticker]["platforms"].add("reddit")
        
        # Convert sets to lists for JSON serialization
        for ticker in trending:
            trending[ticker]["platforms"] = list(trending[ticker]["platforms"])
        
        # Sort by mentions and score
        sorted_tickers = sorted(
            trending.items(),
            key=lambda x: (x[1]["mentions"], x[1]["total_score"]),
            reverse=True
        )[:limit]
        
        result = {
            "trending_tickers": [
                {
                    "ticker": ticker,
                    "mentions": data["mentions"],
                    "total_score": data["total_score"],
                    "platforms": data["platforms"],
                    "company": TICKER_TO_COMPANY.get(ticker, "Unknown")
                }
                for ticker, data in sorted_tickers
            ],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "sources": platforms
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})
