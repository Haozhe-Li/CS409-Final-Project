"""
News and Sentiment Tools for Financial Trading MCP Server
Provides tools for news analysis from Bloomberg, Reuters, FinHub, and other sources
"""
import os
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import httpx
from bs4 import BeautifulSoup
import re

# Configuration
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY", "")
ALPHA_VANTAGE_BASE_URL = "https://www.alphavantage.co/query"
FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY", "")
FINNHUB_BASE_URL = "https://finnhub.io/api/v1"
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")
NEWS_API_BASE_URL = "https://newsapi.org/v2"
BLOOMBERG_API_KEY = os.getenv("BLOOMBERG_API_KEY", "")
REUTERS_API_KEY = os.getenv("REUTERS_API_KEY", "")

async def get_http() -> httpx.AsyncClient:
    """Get or create HTTP client."""
    return httpx.AsyncClient(timeout=30.0)

async def get_stock_news(
    ticker: str,
    start_date: str,
    end_date: str,
    sources: List[str] = None,
    limit: int = 10
) -> str:
    """
    Retrieve news articles for a specific stock from multiple sources.
    
    Args:
        ticker: Ticker symbol (e.g., AAPL, TSLA)
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
        sources: List of news sources to use (alpha_vantage, finnhub, newsapi, bloomberg, reuters)
        limit: Maximum number of articles to return per source
    
    Returns:
        JSON string with news articles including title, summary, source, and sentiment
    """
    try:
        if sources is None:
            sources = ["finnhub", "alpha_vantage", "newsapi"]
        
        all_articles = []
        
        # Finnhub News
        if "finnhub" in sources and FINNHUB_API_KEY:
            try:
                client = await get_http()
                url = f"{FINNHUB_BASE_URL}/company-news"
                params = {
                    "symbol": ticker,
                    "from": start_date,
                    "to": end_date,
                    "token": FINNHUB_API_KEY
                }
                
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    articles = response.json()
                    
                    for article in articles[:limit]:
                        # Simple sentiment analysis based on headline
                        headline = article.get("headline", "").lower()
                        summary = article.get("summary", "").lower()
                        full_text = headline + " " + summary
                        
                        sentiment = analyze_text_sentiment(full_text)
                        
                        all_articles.append({
                            "source": "finnhub",
                            "title": article.get("headline"),
                            "summary": article.get("summary"),
                            "url": article.get("url"),
                            "published": datetime.fromtimestamp(article.get("datetime", 0)).strftime("%Y-%m-%d %H:%M:%S"),
                            "category": article.get("category"),
                            "sentiment": sentiment["sentiment"],
                            "sentiment_score": sentiment["score"]
                        })
            except Exception as e:
                pass  # Continue with other sources
        
        # Alpha Vantage News
        if "alpha_vantage" in sources and ALPHA_VANTAGE_API_KEY:
            try:
                client = await get_http()
                params = {
                    "function": "NEWS_SENTIMENT",
                    "tickers": ticker,
                    "apikey": ALPHA_VANTAGE_API_KEY,
                    "limit": min(limit, 50)
                }
                
                # Add time range if specified
                if start_date:
                    params["time_from"] = start_date.replace("-", "") + "T0000"
                if end_date:
                    params["time_to"] = end_date.replace("-", "") + "T2359"
                
                response = await client.get(ALPHA_VANTAGE_BASE_URL, params=params)
                data = response.json()
                
                if "feed" in data:
                    for item in data["feed"][:limit]:
                        # Extract ticker-specific sentiment
                        ticker_sentiment = None
                        for ts in item.get("ticker_sentiment", []):
                            if ts.get("ticker") == ticker:
                                ticker_sentiment = ts
                                break
                        
                        all_articles.append({
                            "source": "alpha_vantage",
                            "title": item.get("title"),
                            "summary": item.get("summary"),
                            "url": item.get("url"),
                            "published": item.get("time_published"),
                            "sentiment": ticker_sentiment.get("ticker_sentiment_label") if ticker_sentiment else item.get("overall_sentiment_label"),
                            "sentiment_score": float(ticker_sentiment.get("ticker_sentiment_score")) if ticker_sentiment else float(item.get("overall_sentiment_score", 0))
                        })
            except Exception as e:
                pass
        
        # NewsAPI
        if "newsapi" in sources and NEWS_API_KEY:
            try:
                client = await get_http()
                url = f"{NEWS_API_BASE_URL}/everything"
                params = {
                    "q": ticker,
                    "from": start_date,
                    "to": end_date,
                    "sortBy": "relevancy",
                    "pageSize": limit,
                    "apiKey": NEWS_API_KEY
                }
                
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get("articles", [])
                    
                    for article in articles:
                        title = article.get("title", "")
                        description = article.get("description", "")
                        content = article.get("content", "")
                        full_text = f"{title} {description} {content}".lower()
                        
                        sentiment = analyze_text_sentiment(full_text)
                        
                        all_articles.append({
                            "source": "newsapi",
                            "title": title,
                            "summary": description,
                            "url": article.get("url"),
                            "published": article.get("publishedAt"),
                            "source_name": article.get("source", {}).get("name"),
                            "sentiment": sentiment["sentiment"],
                            "sentiment_score": sentiment["score"]
                        })
            except Exception as e:
                pass
        
        # Sort articles by published date
        all_articles.sort(key=lambda x: x.get("published", ""), reverse=True)
        
        # Calculate overall sentiment
        if all_articles:
            positive_count = sum(1 for a in all_articles if a.get("sentiment") in ["Bullish", "positive"])
            negative_count = sum(1 for a in all_articles if a.get("sentiment") in ["Bearish", "negative"])
            neutral_count = sum(1 for a in all_articles if a.get("sentiment") in ["Neutral", "neutral"])
            
            overall_sentiment = "neutral"
            if positive_count > negative_count * 1.5:
                overall_sentiment = "positive"
            elif negative_count > positive_count * 1.5:
                overall_sentiment = "negative"
            
            avg_sentiment_score = sum(a.get("sentiment_score", 0) for a in all_articles) / len(all_articles)
        else:
            overall_sentiment = "no_data"
            avg_sentiment_score = 0
            positive_count = negative_count = neutral_count = 0
        
        result = {
            "symbol": ticker,
            "date_range": f"{start_date} to {end_date}",
            "articles": all_articles[:limit * 2],  # Return more since we're combining sources
            "article_count": len(all_articles),
            "sentiment_analysis": {
                "overall_sentiment": overall_sentiment,
                "average_score": round(avg_sentiment_score, 3),
                "positive_articles": positive_count,
                "negative_articles": negative_count,
                "neutral_articles": neutral_count
            },
            "sources_used": sources,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_market_news(
    topic: str = "general",
    sources: List[str] = None,
    limit: int = 10
) -> str:
    """
    Retrieve general market news and analysis from multiple sources.
    
    Args:
        topic: News topic - "general", "forex", "crypto", "merger", "ipo", "earnings"
        sources: List of news sources to use
        limit: Maximum number of articles to return
    
    Returns:
        JSON string with market news articles
    """
    try:
        if sources is None:
            sources = ["finnhub", "alpha_vantage", "newsapi"]
        
        all_articles = []
        
        # Finnhub Market News
        if "finnhub" in sources and FINNHUB_API_KEY:
            try:
                client = await get_http()
                url = f"{FINNHUB_BASE_URL}/news"
                params = {
                    "category": topic if topic != "general" else "general",
                    "token": FINNHUB_API_KEY
                }
                
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    articles = response.json()
                    
                    for article in articles[:limit]:
                        headline = article.get("headline", "").lower()
                        summary = article.get("summary", "").lower()
                        full_text = headline + " " + summary
                        
                        sentiment = analyze_text_sentiment(full_text)
                        
                        all_articles.append({
                            "source": "finnhub",
                            "title": article.get("headline"),
                            "summary": article.get("summary"),
                            "url": article.get("url"),
                            "published": datetime.fromtimestamp(article.get("datetime", 0)).strftime("%Y-%m-%d %H:%M:%S"),
                            "category": article.get("category"),
                            "sentiment": sentiment["sentiment"],
                            "sentiment_score": sentiment["score"]
                        })
            except Exception as e:
                pass
        
        # Alpha Vantage Market News
        if "alpha_vantage" in sources and ALPHA_VANTAGE_API_KEY:
            try:
                client = await get_http()
                params = {
                    "function": "NEWS_SENTIMENT",
                    "topics": topic,
                    "apikey": ALPHA_VANTAGE_API_KEY,
                    "limit": min(limit, 50)
                }
                
                response = await client.get(ALPHA_VANTAGE_BASE_URL, params=params)
                data = response.json()
                
                if "feed" in data:
                    for item in data["feed"][:limit]:
                        all_articles.append({
                            "source": "alpha_vantage",
                            "title": item.get("title"),
                            "summary": item.get("summary"),
                            "url": item.get("url"),
                            "published": item.get("time_published"),
                            "topics": item.get("topics", []),
                            "sentiment": item.get("overall_sentiment_label"),
                            "sentiment_score": float(item.get("overall_sentiment_score", 0))
                        })
            except Exception as e:
                pass
        
        # NewsAPI Headlines
        if "newsapi" in sources and NEWS_API_KEY:
            try:
                client = await get_http()
                
                # Map topics to NewsAPI categories
                category_map = {
                    "general": "business",
                    "tech": "technology",
                    "crypto": "technology"
                }
                category = category_map.get(topic, "business")
                
                url = f"{NEWS_API_BASE_URL}/top-headlines"
                params = {
                    "category": category,
                    "country": "us",
                    "pageSize": limit,
                    "apiKey": NEWS_API_KEY
                }
                
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    articles = data.get("articles", [])
                    
                    for article in articles:
                        title = article.get("title", "")
                        description = article.get("description", "")
                        full_text = f"{title} {description}".lower()
                        
                        sentiment = analyze_text_sentiment(full_text)
                        
                        all_articles.append({
                            "source": "newsapi",
                            "title": title,
                            "summary": description,
                            "url": article.get("url"),
                            "published": article.get("publishedAt"),
                            "source_name": article.get("source", {}).get("name"),
                            "sentiment": sentiment["sentiment"],
                            "sentiment_score": sentiment["score"]
                        })
            except Exception as e:
                pass
        
        # Sort by published date
        all_articles.sort(key=lambda x: x.get("published", ""), reverse=True)
        
        result = {
            "topic": topic,
            "articles": all_articles[:limit * 2],
            "article_count": len(all_articles),
            "sources_used": sources,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_earnings_calendar(
    start_date: str = None,
    end_date: str = None
) -> str:
    """
    Get upcoming earnings announcements.
    
    Args:
        start_date: Start date in YYYY-MM-DD format (default: today)
        end_date: End date in YYYY-MM-DD format (default: 7 days from start)
    
    Returns:
        JSON string with upcoming earnings announcements
    """
    try:
        if not start_date:
            start_date = datetime.now().strftime("%Y-%m-%d")
        if not end_date:
            end_date = (datetime.now() + timedelta(days=7)).strftime("%Y-%m-%d")
        
        earnings = []
        
        if FINNHUB_API_KEY:
            client = await get_http()
            url = f"{FINNHUB_BASE_URL}/calendar/earnings"
            params = {
                "from": start_date,
                "to": end_date,
                "token": FINNHUB_API_KEY
            }
            
            response = await client.get(url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                earnings_data = data.get("earningsCalendar", [])
                
                for earning in earnings_data:
                    earnings.append({
                        "symbol": earning.get("symbol"),
                        "date": earning.get("date"),
                        "hour": earning.get("hour", "N/A"),
                        "eps_estimate": earning.get("epsEstimate"),
                        "eps_actual": earning.get("epsActual"),
                        "revenue_estimate": earning.get("revenueEstimate"),
                        "revenue_actual": earning.get("revenueActual"),
                        "quarter": earning.get("quarter"),
                        "year": earning.get("year")
                    })
        
        # Sort by date
        earnings.sort(key=lambda x: x.get("date", ""))
        
        result = {
            "date_range": f"{start_date} to {end_date}",
            "earnings_count": len(earnings),
            "earnings": earnings,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

async def get_ipo_calendar(
    start_date: str = None,
    end_date: str = None
) -> str:
    """
    Get upcoming IPO announcements.
    
    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
    
    Returns:
        JSON string with upcoming IPO information
    """
    try:
        if not start_date:
            start_date = datetime.now().strftime("%Y-%m-%d")
        if not end_date:
            end_date = (datetime.now() + timedelta(days=30)).strftime("%Y-%m-%d")
        
        ipos = []
        
        if FINNHUB_API_KEY:
            client = await get_http()
            url = f"{FINNHUB_BASE_URL}/calendar/ipo"
            params = {
                "from": start_date,
                "to": end_date,
                "token": FINNHUB_API_KEY
            }
            
            response = await client.get(url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                ipo_calendar = data.get("ipoCalendar", [])
                
                for ipo in ipo_calendar:
                    ipos.append({
                        "symbol": ipo.get("symbol"),
                        "name": ipo.get("name"),
                        "date": ipo.get("date"),
                        "exchange": ipo.get("exchange"),
                        "price_range": f"{ipo.get('priceRangeLow', 'N/A')} - {ipo.get('priceRangeHigh', 'N/A')}",
                        "shares": ipo.get("numberOfShares"),
                        "total_value": ipo.get("totalSharesValue"),
                        "status": ipo.get("status")
                    })
        
        result = {
            "date_range": f"{start_date} to {end_date}",
            "ipo_count": len(ipos),
            "ipos": ipos,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

def analyze_text_sentiment(text: str) -> Dict[str, Any]:
    """
    Simple sentiment analysis based on keywords.
    
    Args:
        text: Text to analyze
    
    Returns:
        Dictionary with sentiment and score
    """
    text = text.lower()
    
    # Positive keywords
    positive_words = [
        "surge", "gain", "profit", "rally", "bullish", "upgrade", "buy", "growth",
        "positive", "strong", "beat", "exceed", "outperform", "breakthrough", "success",
        "rise", "increase", "up", "higher", "boost", "improve", "record"
    ]
    
    # Negative keywords
    negative_words = [
        "fall", "loss", "decline", "bearish", "downgrade", "sell", "crash",
        "negative", "weak", "miss", "underperform", "concern", "risk", "warning",
        "drop", "decrease", "down", "lower", "cut", "reduce", "plunge"
    ]
    
    positive_score = sum(1 for word in positive_words if word in text)
    negative_score = sum(1 for word in negative_words if word in text)
    
    # Calculate sentiment
    if positive_score > negative_score:
        sentiment = "positive"
        score = min(1.0, positive_score / 10)
    elif negative_score > positive_score:
        sentiment = "negative"
        score = max(-1.0, -negative_score / 10)
    else:
        sentiment = "neutral"
        score = 0.0
    
    return {
        "sentiment": sentiment,
        "score": round(score, 3),
        "positive_words": positive_score,
        "negative_words": negative_score
    }
