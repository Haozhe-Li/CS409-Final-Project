#!/usr/bin/env python3
"""
Reddit MCP Server
Model Context Protocol (MCP) server for Reddit sentiment analysis and social media monitoring.
Provides access to Reddit posts, comments, and sentiment analysis for financial discussions.
"""
import os
import sys
import json
import requests
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from mcp.server.fastmcp import FastMCP

# Create FastMCP server instance
mcp = FastMCP("Reddit MCP Server")

# Configuration
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID", "")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET", "")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "MCP-Reddit-Bot/1.0")

# ============================================================================
# AUTHENTICATION
# ============================================================================

async def get_reddit_token() -> Optional[str]:
    """Get Reddit OAuth token."""
    if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET:
        return None
    
    try:
        auth_url = "https://www.reddit.com/api/v1/access_token"
        auth = (REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET)
        data = {"grant_type": "client_credentials"}
        headers = {"User-Agent": REDDIT_USER_AGENT}
        
        response = requests.post(auth_url, auth=auth, data=data, headers=headers)
        if response.status_code == 200:
            return response.json().get("access_token")
    except:
        pass
    
    return None

# ============================================================================
# REDDIT TOOLS
# ============================================================================

@mcp.tool()
async def search_reddit(
    query: str,
    subreddit: Optional[str] = None,
    sort: str = "relevance",
    time_filter: str = "all",
    limit: int = 25
) -> str:
    """
    Search Reddit posts.
    
    Args:
        query: Search query
        subreddit: Specific subreddit (optional)
        sort: Sort by (relevance, hot, top, new, comments)
        time_filter: Time filter (hour, day, week, month, year, all)
        limit: Maximum results (max 100)
    
    Returns:
        JSON with search results
    """
    try:
        token = await get_reddit_token()
        if not token:
            return json.dumps({"error": "Reddit API credentials not configured"})
        
        headers = {
            "Authorization": f"Bearer {token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        # Build search URL
        if subreddit:
            url = f"https://oauth.reddit.com/r/{subreddit}/search"
        else:
            url = "https://oauth.reddit.com/search"
        
        params = {
            "q": query,
            "sort": sort,
            "t": time_filter,
            "limit": min(limit, 100),
            "restrict_sr": "true" if subreddit else "false"
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            return json.dumps({"error": f"Reddit API error: {response.status_code}"})
        
        data = response.json()
        posts = []
        
        for child in data.get("data", {}).get("children", []):
            post = child.get("data", {})
            posts.append({
                "title": post.get("title"),
                "subreddit": post.get("subreddit"),
                "author": post.get("author"),
                "score": post.get("score"),
                "upvote_ratio": post.get("upvote_ratio"),
                "num_comments": post.get("num_comments"),
                "created_utc": datetime.fromtimestamp(post.get("created_utc", 0)).isoformat(),
                "selftext": post.get("selftext", "")[:500],  # First 500 chars
                "url": f"https://reddit.com{post.get('permalink')}"
            })
        
        result = {
            "query": query,
            "subreddit": subreddit or "all",
            "posts": posts,
            "count": len(posts)
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

@mcp.tool()
async def get_subreddit_posts(
    subreddit: str,
    sort: str = "hot",
    time_filter: str = "day",
    limit: int = 25
) -> str:
    """
    Get posts from a specific subreddit.
    
    Args:
        subreddit: Subreddit name (e.g., wallstreetbets, stocks, investing)
        sort: Sort by (hot, new, top, rising)
        time_filter: For 'top' sort (hour, day, week, month, year, all)
        limit: Maximum posts (max 100)
    
    Returns:
        JSON with subreddit posts
    """
    try:
        token = await get_reddit_token()
        if not token:
            return json.dumps({"error": "Reddit API credentials not configured"})
        
        headers = {
            "Authorization": f"Bearer {token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        url = f"https://oauth.reddit.com/r/{subreddit}/{sort}"
        params = {"limit": min(limit, 100)}
        
        if sort == "top":
            params["t"] = time_filter
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            return json.dumps({"error": f"Reddit API error: {response.status_code}"})
        
        data = response.json()
        posts = []
        
        for child in data.get("data", {}).get("children", []):
            post = child.get("data", {})
            posts.append({
                "title": post.get("title"),
                "author": post.get("author"),
                "score": post.get("score"),
                "upvote_ratio": post.get("upvote_ratio"),
                "num_comments": post.get("num_comments"),
                "created_utc": datetime.fromtimestamp(post.get("created_utc", 0)).isoformat(),
                "flair": post.get("link_flair_text"),
                "selftext": post.get("selftext", "")[:500],
                "url": f"https://reddit.com{post.get('permalink')}"
            })
        
        result = {
            "subreddit": subreddit,
            "sort": sort,
            "posts": posts,
            "count": len(posts)
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

@mcp.tool()
async def get_stock_mentions(
    ticker: str,
    subreddits: Optional[List[str]] = None,
    limit: int = 25
) -> str:
    """
    Get Reddit mentions of a stock ticker.
    
    Args:
        ticker: Stock ticker symbol
        subreddits: List of subreddits to search (default: financial subreddits)
        limit: Maximum posts per subreddit
    
    Returns:
        JSON with stock mentions and sentiment
    """
    try:
        if subreddits is None:
            subreddits = ["wallstreetbets", "stocks", "investing", "StockMarket", "options"]
        
        token = await get_reddit_token()
        if not token:
            return json.dumps({"error": "Reddit API credentials not configured"})
        
        headers = {
            "Authorization": f"Bearer {token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        all_mentions = []
        
        for subreddit in subreddits:
            url = f"https://oauth.reddit.com/r/{subreddit}/search"
            params = {
                "q": ticker,
                "sort": "new",
                "limit": limit,
                "restrict_sr": "true"
            }
            
            response = requests.get(url, headers=headers, params=params)
            
            if response.status_code == 200:
                data = response.json()
                
                for child in data.get("data", {}).get("children", []):
                    post = child.get("data", {})
                    
                    # Simple sentiment based on score and comments
                    sentiment = "neutral"
                    if post.get("score", 0) > 100:
                        sentiment = "bullish"
                    elif post.get("score", 0) < -10:
                        sentiment = "bearish"
                    
                    all_mentions.append({
                        "subreddit": subreddit,
                        "title": post.get("title"),
                        "score": post.get("score"),
                        "num_comments": post.get("num_comments"),
                        "created_utc": datetime.fromtimestamp(post.get("created_utc", 0)).isoformat(),
                        "sentiment": sentiment,
                        "url": f"https://reddit.com{post.get('permalink')}"
                    })
        
        # Calculate overall sentiment
        bullish = sum(1 for m in all_mentions if m["sentiment"] == "bullish")
        bearish = sum(1 for m in all_mentions if m["sentiment"] == "bearish")
        neutral = sum(1 for m in all_mentions if m["sentiment"] == "neutral")
        
        result = {
            "ticker": ticker.upper(),
            "mentions": all_mentions[:50],  # Limit to 50 total
            "count": len(all_mentions),
            "sentiment_summary": {
                "bullish": bullish,
                "bearish": bearish,
                "neutral": neutral,
                "overall": "bullish" if bullish > bearish else "bearish" if bearish > bullish else "neutral"
            }
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

@mcp.tool()
async def get_trending_tickers(
    subreddit: str = "wallstreetbets",
    time_filter: str = "day",
    limit: int = 100
) -> str:
    """
    Get trending stock tickers from Reddit.
    
    Args:
        subreddit: Subreddit to analyze
        time_filter: Time period (hour, day, week)
        limit: Number of posts to analyze
    
    Returns:
        JSON with trending tickers and mention counts
    """
    try:
        token = await get_reddit_token()
        if not token:
            return json.dumps({"error": "Reddit API credentials not configured"})
        
        headers = {
            "Authorization": f"Bearer {token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        url = f"https://oauth.reddit.com/r/{subreddit}/hot"
        params = {"limit": limit}
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            return json.dumps({"error": f"Reddit API error: {response.status_code}"})
        
        data = response.json()
        
        # Common stock tickers pattern
        import re
        ticker_pattern = r'\b[A-Z]{2,5}\b'
        
        # Common tickers to track
        common_tickers = {
            "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "META", "NVDA", "AMD",
            "GME", "AMC", "BB", "NOK", "PLTR", "NIO", "SPCE", "WISH", "CLOV",
            "SPY", "QQQ", "IWM", "DIA", "VIX", "GLD", "SLV", "USO",
            "COIN", "RIOT", "MARA", "SQ", "PYPL", "SOFI", "HOOD", "LCID"
        }
        
        ticker_counts = {}
        
        for child in data.get("data", {}).get("children", []):
            post = child.get("data", {})
            text = f"{post.get('title', '')} {post.get('selftext', '')}"
            
            # Find all potential tickers
            potential_tickers = re.findall(ticker_pattern, text)
            
            for ticker in potential_tickers:
                if ticker in common_tickers:
                    ticker_counts[ticker] = ticker_counts.get(ticker, 0) + 1
        
        # Sort by count
        trending = sorted(ticker_counts.items(), key=lambda x: x[1], reverse=True)
        
        result = {
            "subreddit": subreddit,
            "time_filter": time_filter,
            "posts_analyzed": limit,
            "trending_tickers": [
                {"ticker": t[0], "mentions": t[1]} for t in trending[:20]
            ]
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

@mcp.tool()
async def get_dd_posts(
    subreddit: str = "wallstreetbets",
    limit: int = 10
) -> str:
    """
    Get Due Diligence (DD) posts from Reddit.
    
    Args:
        subreddit: Subreddit to search
        limit: Maximum DD posts
    
    Returns:
        JSON with DD posts
    """
    try:
        token = await get_reddit_token()
        if not token:
            return json.dumps({"error": "Reddit API credentials not configured"})
        
        headers = {
            "Authorization": f"Bearer {token}",
            "User-Agent": REDDIT_USER_AGENT
        }
        
        # Search for DD flair
        url = f"https://oauth.reddit.com/r/{subreddit}/search"
        params = {
            "q": "flair:DD OR flair:\"Due Diligence\"",
            "sort": "top",
            "t": "week",
            "limit": limit,
            "restrict_sr": "true"
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            return json.dumps({"error": f"Reddit API error: {response.status_code}"})
        
        data = response.json()
        dd_posts = []
        
        for child in data.get("data", {}).get("children", []):
            post = child.get("data", {})
            
            # Extract ticker mentions from title
            import re
            tickers = re.findall(r'\b[A-Z]{2,5}\b', post.get("title", ""))
            
            dd_posts.append({
                "title": post.get("title"),
                "author": post.get("author"),
                "score": post.get("score"),
                "upvote_ratio": post.get("upvote_ratio"),
                "awards": post.get("total_awards_received"),
                "num_comments": post.get("num_comments"),
                "created_utc": datetime.fromtimestamp(post.get("created_utc", 0)).isoformat(),
                "tickers_mentioned": list(set(tickers)),
                "preview": post.get("selftext", "")[:500],
                "url": f"https://reddit.com{post.get('permalink')}"
            })
        
        result = {
            "subreddit": subreddit,
            "dd_posts": dd_posts,
            "count": len(dd_posts)
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": str(e)})

def main():
    """Main entry point for the MCP server."""
    print("Starting Reddit MCP Server...", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    
    if REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET:
        print("✓ API credentials configured", file=sys.stderr)
    else:
        print("✗ API credentials not configured", file=sys.stderr)
        print("  Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET", file=sys.stderr)
        print("  Get credentials at: https://www.reddit.com/prefs/apps", file=sys.stderr)
    
    print("\nAvailable Tools:", file=sys.stderr)
    print("  Search: search_reddit", file=sys.stderr)
    print("  Subreddit: get_subreddit_posts", file=sys.stderr)
    print("  Stocks: get_stock_mentions, get_trending_tickers", file=sys.stderr)
    print("  Analysis: get_dd_posts", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    
    sys.stderr.flush()
    mcp.run(transport="stdio")

if __name__ == "__main__":
    main()
